name: CD

on:
  repository_dispatch:
    types: [deploy-app]
  workflow_dispatch:
    inputs:
      image_tag:
        description: "Image tag to deploy (leave empty to auto-pick latest)"
        required: false
        default: ""
      ecr_repository:
        description: "ECR repository name"
        required: true
        default: "fightingcharacters"
      region:
        description: "AWS region (override default if needed)"
        required: true
        default: "ap-south-1"

permissions:
  id-token: write
  contents: read

concurrency:
  group: "${{ github.workflow }}-${{ github.ref }}"
  cancel-in-progress: true

env:
  ACCOUNT_ID: "372595555088"
  AWS_REGION: "ap-south-1"              # set your real region
  AWS_DEFAULT_REGION: "ap-south-1"
  CLUSTER_NAME: "benshavit-cluster"     # set your real cluster name
  K8S_NAMESPACE: "fighting-characters"
  DEPLOYMENT_NAME: "fighting-characters"
  CONTAINER_NAME: "fighting-characters"
  ECR_REPOSITORY: "fightingcharacters"

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 25

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # -------- Resolve inputs/payload --------
      - name: Resolve inputs/payload
        id: resolve
        shell: bash
        run: |
          set -euo pipefail
          export AWS_PAGER=""

          if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            IMAGE_TAG_IN="${{ github.event.client_payload.image_tag }}"
            ECR_REPO_IN="${{ github.event.client_payload.ecr_repository }}"
            AWS_REGION_IN="${{ github.event.client_payload.region }}"
          else
            IMAGE_TAG_IN="${{ inputs.image_tag }}"
            ECR_REPO_IN="${{ inputs.ecr_repository }}"
            AWS_REGION_IN="${{ inputs.region }}"
          fi

          if [ -n "${AWS_REGION_IN}" ]; then
            echo "AWS_REGION=${AWS_REGION_IN}"       >> "$GITHUB_ENV"
            echo "AWS_DEFAULT_REGION=${AWS_REGION_IN}" >> "$GITHUB_ENV"
          fi
          if [ -n "${IMAGE_TAG_IN}" ]; then
            echo "IMAGE_TAG=${IMAGE_TAG_IN}" >> "$GITHUB_ENV"
          fi
          if [ -n "${ECR_REPO_IN}" ]; then
            echo "ECR_REPOSITORY=${ECR_REPO_IN}" >> "$GITHUB_ENV"
          fi

          echo "image_tag=${IMAGE_TAG_IN}"     >> "$GITHUB_OUTPUT"
          echo "ecr_repository=${ECR_REPO_IN}" >> "$GITHUB_OUTPUT"
          echo "aws_region=${AWS_REGION_IN}"   >> "$GITHUB_OUTPUT"

      # -------- AWS auth (OIDC) --------
      - name: Configure AWS (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.ACCOUNT_ID }}:role/gh-oidc-eks-deploy
          aws-region: ${{ env.AWS_REGION }}

      - name: Who am I & context
        shell: bash
        run: |
          set -euo pipefail
          export AWS_PAGER=""
          echo "ACCOUNT_ID: ${ACCOUNT_ID}"
          echo "AWS_REGION: ${AWS_REGION}"
          echo "CLUSTER_NAME: ${CLUSTER_NAME}"
          aws sts get-caller-identity
          
          sleep 5

      # -------- Tooling --------
      - name: Install kubectl (stable)
        shell: bash
        run: |
          set -euo pipefail
          curl -sSL -o kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -sSL https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      - name: Install Helm
        shell: bash
        run: |
          set -euo pipefail
          curl -sSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version

      - name: Install kustomize
        shell: bash
        run: |
          set -euo pipefail
          KUSTOMIZE_VERSION=5.4.1
          curl -sSL -o kustomize.tar.gz "https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv${KUSTOMIZE_VERSION}/kustomize_v${KUSTOMIZE_VERSION}_linux_amd64.tar.gz"
          tar -xzf kustomize.tar.gz
          sudo mv kustomize /usr/local/bin/kustomize
          kustomize version

      # -------- Kubeconfig & basic access --------
      - name: Update kubeconfig & sanity
        shell: bash
        run: |
          set -euo pipefail
          export AWS_PAGER=""
          aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" >/dev/null
          aws eks update-kubeconfig --region "${AWS_REGION}" --name "${CLUSTER_NAME}"
          kubectl cluster-info
          kubectl get nodes -o wide || true
          kubectl get ns || true

      # -------- Resolve image tag from ECR if empty --------
      - name: Auto-pick latest image tag if missing
        if: env.IMAGE_TAG == ''
        shell: bash
        run: |
          set -euo pipefail
          TAG=$(aws ecr describe-images --repository-name "${ECR_REPOSITORY}" --region "${AWS_REGION}" \
                --query "reverse(sort_by(imageDetails,&imagePushedAt))[0].imageTags[0]" --output text)
          [ -z "${TAG}" -o "${TAG}" = "None" ] && { echo "No tags found in ECR '${ECR_REPOSITORY}'"; exit 1; }
          echo "IMAGE_TAG=${TAG}" >> "$GITHUB_ENV"
          echo "Using latest image tag: ${TAG}"

      # -------- Ensure AWS Load Balancer Controller (best-effort) --------
      - name: Ensure AWS Load Balancer Controller healthy
        shell: bash
        run: |
          set -euo pipefail
          ns="kube-system"

          heal_controller() {
            echo "Discovering VPC ID from cluster subnets ..."
            SUB1=$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
                    --query 'cluster.resourcesVpcConfig.subnetIds[0]' --output text)
            [ -z "${SUB1}" -o "${SUB1}" = "None" ] && { echo "::warning::Unable to read subnets from EKS cluster config."; return 0; }
            VPC_ID=$(aws ec2 describe-subnets --subnet-ids "${SUB1}" --region "${AWS_REGION}" \
                     --query 'Subnets[0].VpcId' --output text)
            [ -z "${VPC_ID}" -o "${VPC_ID}" = "None" ] && { echo "::warning::Failed to resolve VPC ID"; return 0; }

            echo "VPC_ID resolved: ${VPC_ID}"
            echo "Installing/Upgrading ALB Controller via Helm (reuse IRSA if exists)..."
            helm repo add eks https://aws.github.io/eks-charts
            helm repo update

            helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
              --namespace "${ns}" \
              --set clusterName="${CLUSTER_NAME}" \
              --set region="${AWS_REGION}" \
              --set vpcId="${VPC_ID}" \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller
          }

          wait_ready() {
            kubectl -n "${ns}" rollout status deploy/aws-load-balancer-controller --timeout=5m
            echo "Waiting for webhook endpoints..."
            for i in {1..60}; do
              EPS=$(kubectl -n "${ns}" get endpoints aws-load-balancer-webhook-service \
                    -o jsonpath='{.subsets[*].addresses[*].ip}' 2>/dev/null || true)
              [ -n "${EPS}" ] && { echo "Webhook endpoints are ready: ${EPS}"; return 0; }
              sleep 5
            done
            return 1
          }

          echo "Checking ALB Controller state ..."
          if ! kubectl -n "${ns}" get deploy aws-load-balancer-controller >/dev/null 2>&1; then
            echo "Controller not found -> installing."
            heal_controller || true
          fi

          if ! wait_ready; then
            echo "::notice::ALB Controller not ready yet; continuing. Ingress hostname step may be pending."
          else
            echo "ALB Controller is healthy."
          fi

      # -------- Apply manifests --------
      - name: Apply manifests with Kustomize
        shell: bash
        run: |
          set -euo pipefail
          kubectl create namespace "${K8S_NAMESPACE}" --dry-run=client -o yaml | kubectl apply -f -
          kustomize build --enable-helm k8/ | kubectl apply -n "${K8S_NAMESPACE}" -f -

      # -------- Set image on Deployment --------
      - name: Set image on Deployment
        env:
          FULL_IMAGE: ${{ env.ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}
        shell: bash
        run: |
          set -euo pipefail
          echo "Setting image: ${FULL_IMAGE}"
          kubectl -n "${K8S_NAMESPACE}" set image deployment/"${DEPLOYMENT_NAME}" "${CONTAINER_NAME}=${FULL_IMAGE}"

      # -------- Zero-downtime rollout (+ fallback) --------
      - name: Zero-downtime rollout with fallback
        shell: bash
        run: |
          set -euo pipefail
          ns="${K8S_NAMESPACE}"
          dep="${DEPLOYMENT_NAME}"
          orig=$(kubectl -n "${ns}" get deploy "${dep}" -o jsonpath='{.spec.replicas}' 2>/dev/null || echo 1)
          [ -z "${orig}" ] && orig=1
          echo "Current replicas: ${orig}"
          kubectl -n "${ns}" patch deploy "${dep}" -p '{"spec":{"strategy":{"type":"RollingUpdate","rollingUpdate":{"maxUnavailable":0,"maxSurge":1}}}}' --type=merge
          if [ "${orig}" -lt 2 ]; then
            echo "Scaling up temporarily to 2 replicas."
            kubectl -n "${ns}" scale deploy "${dep}" --replicas=2
          fi
          echo "Waiting for rollout..."
          if kubectl -n "${ns}" rollout status deploy/"${dep}" --timeout=300s; then
            echo "Rollout completed."
            if [ "${orig}" -lt 2 ]; then
              echo "Scaling back to ${orig}."
              kubectl -n "${ns}" scale deploy "${dep}" --replicas="${orig}"
            fi
          else
            echo "Rollout failed. Attempting rollback to previous revision."
            kubectl -n "${ns}" rollout undo deploy/"${dep}" || true
            echo "--- Diagnostics ---"
            kubectl -n "${ns}" get ing -o wide || true
            kubectl -n "${ns}" describe ing || true
            kubectl -n "${ns}" get svc -o wide || true
            kubectl -n "${ns}" get deploy "${dep}" -o wide || true
            kubectl -n "${ns}" get rs -l app="${dep}" -o wide || true
            kubectl -n "${ns}" get pods -l app="${dep}" -o wide || true
            kubectl -n "${ns}" describe deploy "${dep}" | sed -n '/Events:/,$p' || true
            POD="$(kubectl -n "${ns}" get pods -l app="${dep}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
            if [ -n "${POD}" ]; then
              kubectl -n "${ns}" describe pod "${POD}" | sed -n '/Events:/,$p' || true
              kubectl -n "${ns}" logs "${POD}" --tail=200 || true
            fi
            exit 1
          fi

      # -------- Wait for ALB hostname (best-effort) --------
      - name: Wait ≤120s for ALB hostname & print URL (best-effort)
        shell: bash
        run: |
          set -euo pipefail
          ns="${K8S_NAMESPACE}"
          ing="fighting-characters"

          if ! kubectl -n "${ns}" get ingress "${ing}" >/dev/null 2>&1; then
            ing="$(kubectl -n "${ns}" get ing -l app=fighting-characters -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
          fi

          if [ -z "${ing}" ]; then
            echo "::notice title=Ingress missing::No Ingress found for app=fighting-characters"
            kubectl -n "${ns}" get ing || true
            exit 0
          fi

          echo "Waiting up to 120s for ALB hostname on Ingress/${ing} ..."
          host=""
          for i in {1..24}; do
            host=$(kubectl -n "${ns}" get ingress "${ing}" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            [ -n "${host}" ] && break
            sleep 5
          done

          if [ -z "${host}" ]; then
            echo "::notice title=ALB pending::Ingress has no hostname after 120s."
            kubectl -n "${ns}" describe ingress "${ing}" || true
            POD=$(kubectl -n kube-system get pods -l app.kubernetes.io/name=aws-load-balancer-controller -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
            [ -n "${POD}" ] && kubectl -n kube-system logs "${POD}" --tail=200 || true
            exit 0
          fi

          url="http://${host}"
          echo "APP_URL=${url}" >> "$GITHUB_ENV"
          echo "App URL: ${url}" | tee -a "$GITHUB_STEP_SUMMARY"
          if command -v curl >/dev/null 2>&1; then
            echo "Probing ${url}/health ..."
            curl -fsS "${url}/health" >/dev/null && echo "Health: OK" || echo "::warning::/health probe failed"
          fi

